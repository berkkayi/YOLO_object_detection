{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp-0xxLDuy9V"
      },
      "source": [
        "#Preprocessing Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myseD0p6x1wt"
      },
      "source": [
        "##Drive Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3LImM8Brv2B"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL-9_gJhutlL"
      },
      "source": [
        "%cd /content/drive/My Drive/yeni_proje/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFQWS0BTqjJV"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Iq-cs8otQ8"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def preprocess():\n",
        "    # Put first column (id) and second column (name) from category.txt into two lists\n",
        "    category_ids = []\n",
        "    category_names = []\n",
        "    with open(my_dataset_disk + '/' + category, 'r') as category_list:\n",
        "        for i, line in enumerate(category_list):\n",
        "            if i > 0:\n",
        "                line = line.rstrip('\\n')  # delete \\n in the end of th\n",
        "                # e line\n",
        "                line = line.split('\\t')\n",
        "                category_ids.append(int(line[0]))\n",
        "                category_names.append(line[1])\n",
        "\n",
        "    for id_index, id in enumerate(category_ids):\n",
        "        new_bbox_info = []\n",
        "        with open(my_dataset_disk + '/' + str(id) + '/' + bbox_info, 'r') as bbox_list:\n",
        "            for i, line in enumerate(bbox_list):\n",
        "                if i > 0:\n",
        "                    line = line.rstrip('\\n')\n",
        "                    line = line.split(' ')\n",
        "                    img_path = my_dataset_disk + '/' + str(id) + '/' + str(line[0]) + '.jpg'\n",
        "                    ori_img = Image.open(img_path)\n",
        "                    if ori_img.size != (800, 600):\n",
        "                        new_bbox = list(map(str, newbbox(ori_img.size, list(map(int, line[1:])), target_wh)))\n",
        "                        new_bbox.insert(0, str(line[0]))\n",
        "                        new_bbox_info.append(new_bbox)\n",
        "                        ori_img = ori_img.resize([800, 600], Image.ANTIALIAS)\n",
        "                        ori_img.save(img_path)\n",
        "                    else:\n",
        "                        new_bbox_info.append(line)\n",
        "        rewrite_bbox(id, new_bbox_info)\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "def newbbox(ori_size, oldbbox, target_wh):\n",
        "    new_bbox = np.squeeze(np.zeros((1, 4)))\n",
        "    wscale = target_wh[0] / ori_size[0]\n",
        "    hscale = target_wh[1] / ori_size[1]\n",
        "    new_bbox[0], new_bbox[2] = int(wscale * oldbbox[0]), int(wscale * oldbbox[2])\n",
        "    new_bbox[1], new_bbox[3] = int(hscale * oldbbox[1]), int(hscale * oldbbox[3])\n",
        "\n",
        "    if new_bbox[0] > 800:\n",
        "        new_bbox[0] = 800\n",
        "    if new_bbox[2] > 800:\n",
        "        new_bbox[2] = 800\n",
        "    if new_bbox[1] > 600:\n",
        "        new_bbox[1] = 600\n",
        "    if new_bbox[3] > 600:\n",
        "        new_bbox[3] = 600\n",
        "\n",
        "    assert (new_bbox[0], new_bbox[2] <= [target_wh[0], target_wh[0]])[1].all()\n",
        "    assert (new_bbox[1], new_bbox[1] <= [target_wh[1], target_wh[1]])[1].all()\n",
        "\n",
        "    return new_bbox\n",
        "\n",
        "def rewrite_bbox(id, new_bbox_info):\n",
        "    print('rewriting category ' + str(id) + ' ...')\n",
        "    file = open(my_dataset_disk + '/' + str(id) + '/' + 'new_bb_info.txt', 'w')\n",
        "    file.write('img x1 y1 x2 y2\\n')  # header\n",
        "    for i in new_bbox_info:\n",
        "        file.write(i[0] + ' ' + i[1] + ' ' + i[2] + ' ' + i[3] + ' ' + i[4] + '\\n')\n",
        "    file.close()\n",
        "\n",
        "target_wh = [800, 600]\n",
        "my_dataset_disk = '/content/drive/My Drive/yeni_proje/data'\n",
        "category = 'category.txt'\n",
        "bbox_info = 'bb_info.txt'\n",
        "preprocess()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFlPktiVqy3f"
      },
      "source": [
        "##Train Test Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaSgRTcjotTI"
      },
      "source": [
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "def split_dataset():\n",
        "    dataset_disk = '//content/drive/My Drive/proje'\n",
        "#     uecfood100_path = dataset_disk + 'UECFOOD100_448'\n",
        "    uecfood100_path = dataset_disk + '/data'\n",
        "    category = 'category.txt'\n",
        "    bbox_info = 'new_bb_info.txt'\n",
        "\n",
        "    split = [0.7, 0.2, 0.1]\n",
        "    files_generated = ['train_uec100.txt', 'val_uec100.txt', 'test_uec100.txt']\n",
        "\n",
        "    # Put first column (id) and second column (name) from category.txt into two lists\n",
        "    category_ids = []\n",
        "    category_names = []\n",
        "    with open(uecfood100_path + '/' + category, 'r') as category_list:\n",
        "        for i, line in enumerate(category_list):\n",
        "            if i > 0:\n",
        "                line = line.rstrip('\\n')  # delete \\n in the end of th\n",
        "                # e line\n",
        "                line = line.split('\\t')\n",
        "                category_ids.append(int(line[0]))\n",
        "                category_names.append(line[1])\n",
        "\n",
        "    # Read bb_info.txt based on category id\n",
        "    category_images = []\n",
        "    category_bbox = []\n",
        "    for id_index, id in enumerate(category_ids):\n",
        "        category_images.append([])\n",
        "        category_bbox.append([])\n",
        "        with open(uecfood100_path + '/' + str(id) + '/' + bbox_info, 'r') as bbox_list:\n",
        "            for i, line in enumerate(bbox_list):\n",
        "                if i > 0:\n",
        "                    line = line.rstrip('\\n')\n",
        "                    line = line.split(' ')\n",
        "                    category_images[id_index].append(line[0])\n",
        "                    category_bbox[id_index].append(list(map(float, line[1:])))\n",
        "\n",
        "    # Split categories to train/val/test with ratio define before\n",
        "    train_uec100 = []\n",
        "    val_uec100 = []\n",
        "    test_uec100 = []\n",
        "    for id_index, id in enumerate(category_ids):\n",
        "        # divide each category with 70% training, 20% val, 10% testing\n",
        "        n_imgs = len(category_images[id_index])\n",
        "        n_train = int(np.floor(n_imgs * split[0]))\n",
        "        n_val = int(np.floor(n_imgs * split[1]))\n",
        "        n_test = int(n_imgs - n_train - n_val)\n",
        "\n",
        "        # shuffle images\n",
        "        shuffled_imgs = random.sample(category_images[id_index], n_imgs)\n",
        "\n",
        "        train_uec100.append(shuffled_imgs[:n_train])  # not including the last one\n",
        "        val_uec100.append(shuffled_imgs[n_train:n_train + n_val])\n",
        "        test_uec100.append(shuffled_imgs[n_train + n_val:])\n",
        "\n",
        "    all_train_list = list(np.unique(list(itertools.chain(*train_uec100))))\n",
        "    all_val_list = list(np.unique(list(itertools.chain(*val_uec100))))\n",
        "    all_test_list = list(np.unique(list(itertools.chain(*test_uec100))))\n",
        "\n",
        "    # Pop out element in training set if it's in testing or val also\n",
        "    i = 0\n",
        "    while i < len(all_train_list):  # give priority to val and test over train\n",
        "        if all_train_list[i] in all_val_list:  # training sample is in val set too\n",
        "            all_train_list.pop(i)\n",
        "        elif all_train_list[i] in all_test_list:  # training sample is in test set too\n",
        "            all_train_list.pop(i)\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    # Pop out element in testing set if it's in val also\n",
        "    i = 0\n",
        "    while i < len(all_test_list):  # give priority to val over test\n",
        "        if all_test_list[i] in all_val_list:  # test sample is in val set too\n",
        "            all_test_list.pop(i)\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    # Split bounding box with train, val, test sets\n",
        "    imgs_format = 'jpg'\n",
        "    file = open(uecfood100_path + '/classes.txt', 'w')\n",
        "    for c in category_names:\n",
        "        file.write(c + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    #### Training set\n",
        "    file = open(uecfood100_path + '/' + files_generated[0], 'w')\n",
        "    file.write('img category_id x1 y1 x2 y2\\n')  # header\n",
        "    for img in all_train_list:\n",
        "        # it is possible that one image in several categories\n",
        "        occurrences = []\n",
        "        for id_index, id in enumerate(category_ids):\n",
        "            occ = [[uecfood100_path + '/' + str(id) + '/' + img + '.' + imgs_format, str(id)] +\n",
        "                   category_bbox[id_index][i] for i, elem in enumerate(category_images[id_index]) if elem == img]\n",
        "            occurrences += occ\n",
        "\n",
        "        for occ in occurrences:\n",
        "            img_path = occ[0]\n",
        "            img_category = occ[1]\n",
        "            img_bbox = str(occ[2]) + ' ' + str(occ[3]) + ' ' + str(occ[4]) + ' ' + str(occ[5])\n",
        "            file.write(img_path + ' ' + img_category + ' ' + img_bbox + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    #### Val set\n",
        "    file = open(uecfood100_path + '/' + files_generated[1], 'w')\n",
        "    file.write('img category_id x1 y1 x2 y2\\n')  # header\n",
        "    for img in all_train_list:\n",
        "        # it is possible that one image in several categories\n",
        "        occurrences = []\n",
        "        for id_index, id in enumerate(category_ids):\n",
        "            occ = [[uecfood100_path + '/' + str(id) + '/' + img + '.' + imgs_format, str(id)] +\n",
        "                   category_bbox[id_index][i] for i, elem in enumerate(category_images[id_index]) if elem == img]\n",
        "            occurrences += occ\n",
        "\n",
        "        for occ in occurrences:\n",
        "            img_path = occ[0]\n",
        "            img_category = occ[1]\n",
        "            img_bbox = str(occ[2]) + ' ' + str(occ[3]) + ' ' + str(occ[4]) + ' ' + str(occ[5])\n",
        "            file.write(img_path + ' ' + img_category + ' ' + img_bbox + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    #### Testing set\n",
        "    file = open(uecfood100_path + '/' + files_generated[2], 'w')\n",
        "    file.write('img category_id x1 y1 x2 y2\\n')  # header\n",
        "    for img in all_train_list:\n",
        "        # it is possible that one image in several categories\n",
        "        occurrences = []\n",
        "        for id_index, id in enumerate(category_ids):\n",
        "            occ = [[uecfood100_path + '/' + str(id) + '/' + img + '.' + imgs_format, str(id)] +\n",
        "                   category_bbox[id_index][i] for i, elem in enumerate(category_images[id_index]) if elem == img]\n",
        "            occurrences += occ\n",
        "\n",
        "        for occ in occurrences:\n",
        "            img_path = occ[0]\n",
        "            img_category = occ[1]\n",
        "            img_bbox = str(occ[2]) + ' ' + str(occ[3]) + ' ' + str(occ[4]) + ' ' + str(occ[5])\n",
        "            file.write(img_path + ' ' + img_category + ' ' + img_bbox + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    print('Done!')\n",
        "\n",
        "split_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cqVQGwarTLe"
      },
      "source": [
        "##Anchor Box (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yQarNdUotVk"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n",
        "def kmeans(wh, centroids, anchor_txt):\n",
        "    num = wh.shape[0]  # total number of different wh pairs\n",
        "    k, dim = centroids.shape\n",
        "    iter = 0\n",
        "    old_distances = np.zeros((num, k))\n",
        "    _assignments = -np.ones(num)\n",
        "\n",
        "    # iterate until\n",
        "    while True:\n",
        "        new_distances = []\n",
        "        iter += 1\n",
        "        for i in range(num):\n",
        "            distance = 1 - IoU(wh[i], centroids)  # high IoU represents low distance\n",
        "            new_distances.append(distance)\n",
        "        new_distances = np.array(new_distances)\n",
        "        print('Iter {}: distances: {}'.format(iter, np.sum((np.abs(old_distances - new_distances)))))\n",
        "\n",
        "        # for each input img assign a centroid (select the closed one)\n",
        "        assignments = np.argmin(new_distances, axis=1)\n",
        "        if (assignments == _assignments).all():\n",
        "            print('final centroids =', centroids)\n",
        "            save_anchors(centroids, anchor_txt, wh_in_mobilenet)\n",
        "            return centroids\n",
        "        else:\n",
        "            centroid_sums = np.zeros((k, dim), np.float)\n",
        "            for i in range(num):\n",
        "                centroid_sums[assignments[i]] += wh[i]  # sum up attribute\n",
        "            for j in range(k):\n",
        "                # new centroids\n",
        "                centroids[j] = centroid_sums[j] / np.sum(assignments == j)\n",
        "\n",
        "            _assignments = assignments.copy()\n",
        "            old_distances = new_distances.copy()\n",
        "\n",
        "def save_anchors(centroids, anchor_txt, wh_in_mobilenet):\n",
        "    width_in_yolov2 = wh_in_mobilenet[0]\n",
        "    height_in_yolov2 = wh_in_mobilenet[1]\n",
        "    with open(anchor_txt, 'w') as file:\n",
        "        anchors = centroids.copy()\n",
        "        for i in range(anchors.shape[0]):\n",
        "            anchors[i][0] *= width_in_yolov2 / 32.\n",
        "            anchors[i][1] *= height_in_yolov2 / 32.\n",
        "        widths = anchors[:, 0]\n",
        "        sorted_indices = np.argsort(widths)  # return the indices that sort tht array\n",
        "        print('anchors = ', anchors[sorted_indices])\n",
        "\n",
        "        for i in sorted_indices:\n",
        "            file.write('%0.2f, %0.2f\\n' % (anchors[i, 0], anchors[i, 1]))\n",
        "\n",
        "def avgIoU(wh, centroids):\n",
        "    sum = 0.\n",
        "    for i in range(wh.shape[0]):\n",
        "        sum += max(IoU(wh[i], centroids))\n",
        "    return sum / wh.shape[0]\n",
        "\n",
        "def IoU(whi, centroids):\n",
        "    \"\"\" Calculate IoU between current centroids with one in wh array to check if current\n",
        "    centroids are suitable enough\n",
        "    :param whi:\n",
        "    :param centroids:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    IOU = []\n",
        "    for centroid in centroids:\n",
        "        c_w, c_h = centroid\n",
        "        w, h = whi\n",
        "        if c_w >= w and c_h >= h:\n",
        "            iou = w * h / (c_w * c_h)\n",
        "        elif c_w >= w and c_h <= h:\n",
        "            iou = w * c_h / (w * h + (c_w - w) * c_h)\n",
        "        elif c_w <= w and c_h >= h:\n",
        "            iou = c_w * h / (w * h + (c_h - h) * c_w)\n",
        "        else:\n",
        "            iou = c_w * c_h / (w * h)\n",
        "        IOU.append(iou)\n",
        "    return np.array(IOU)\n",
        "\n",
        "def coordinate2wh(coordinates, uec100_dims):\n",
        "    coordinates = list(map(float, coordinates))\n",
        "    w = (coordinates[2] - coordinates[0]) / uec100_dims[0]  # x2-x1\n",
        "    h = (coordinates[3] - coordinates[1]) / uec100_dims[0]  # y2-y1\n",
        "    return w, h\n",
        "\n",
        "def gen_anchors(n_clusters, uec100_dims):\n",
        "#     dataset_disk = '/Volumes/JS/UECFOOD100_448/'\n",
        "    dataset_disk = '/content/drive/My Driver/proje/data/'\n",
        "    output_path = dataset_disk + 'generated_anchors_mobilenet'\n",
        "    train_uec100 = dataset_disk + 'train_uec100.txt'\n",
        "\n",
        "    #if not os.path.exists(output_path):\n",
        "    #    os.mkdir(output_path)\n",
        "\n",
        "    wh = []\n",
        "\n",
        "    with open(train_uec100, 'r') as file:\n",
        "        for i, line in enumerate(file):\n",
        "            if i > 0:\n",
        "                line = line.rstrip('\\n')\n",
        "                line = line.split(' ')\n",
        "                coordinates = line[2:]\n",
        "                w, h = coordinate2wh(coordinates, uec100_dims)\n",
        "                wh.append([w, h])\n",
        "        wh = np.array(wh)\n",
        "\n",
        "        if n_clusters == 0:  # make from 1 to 10 clusters and pick the best one\n",
        "            avgIou = []\n",
        "            for n_cluster in range(1, 11):\n",
        "                anchor_txt = os.path.join(output_path, 'anchors_%d.txt' % (n_cluster))\n",
        "                # randomly select n_cluster anchors from wh array which contain w,h for each img\n",
        "                indices = [random.randrange(wh.shape[0]) for i in range(n_cluster)]\n",
        "                centroids = wh[indices]\n",
        "                centroids = kmeans(wh, centroids, anchor_txt)\n",
        "                avgIou.append([n_cluster, avgIoU(wh, centroids)])\n",
        "            avgIou = np.array(avgIou)\n",
        "            plt.plot(avgIou[:, 0], avgIou[:, 1])\n",
        "            plt.scatter(avgIou[:, 0], avgIou[:, 1], c='r')\n",
        "            plt.xlabel('number of cluster')\n",
        "            plt.ylabel('average IoU')\n",
        "            plt.savefig('avg_iou')\n",
        "            plt.show()\n",
        "        else:\n",
        "            anchor_txt = os.path.join(output_path, 'anchors_%d.txt' % (n_clusters))\n",
        "            # randomly select n_cluster anchors from wh array which contain w,h for each img\n",
        "            indices = [random.randrange(wh.shape[0]) for i in range(n_clusters)]\n",
        "            centroids = wh[indices]\n",
        "            kmeans(wh, centroids, anchor_txt)\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "\n",
        "wh_in_mobilenet = [224, 224]\n",
        "uec256_dims = [800, 600]  # dataset image width=800, height=600\n",
        "n_clusters = 0\n",
        "\n",
        "gen_anchors(n_clusters, uec256_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkGVlwc1otYf"
      },
      "source": [
        "def visualize(wh_in_mobilenet):\n",
        "    stride = 32\n",
        "\n",
        "    colors = [(255, 0, 0), (255, 255, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255), (55, 0, 0),\n",
        "              (255, 55, 0), (0, 55, 0), (0, 0, 25), (0, 255, 55)]\n",
        "#     colors = (0, 0, 0)\n",
        "\n",
        "    anchor_path = dataset_disk + 'generated_anchors_mobilenet/'\n",
        "    for i in range(1, 11):\n",
        "        cv2.namedWindow('%d_anchors' % int(i))\n",
        "        cv2.moveWindow('%d_anchors' % int(i), 100, 100)\n",
        "        blank_image = np.zeros((wh_in_mobilenet[0], wh_in_mobilenet[1], 3), np.uint8)\n",
        "#         blank_image += 255\n",
        "        anchor_txt = anchor_path + 'anchors_%d.txt' % int(i)\n",
        "        anchors = []\n",
        "        with open(anchor_txt, 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.rstrip('\\n')\n",
        "                anchor = line.split(', ')\n",
        "                anchors.append(anchor)\n",
        "\n",
        "        output_img = os.path.join(anchor_txt).replace('.txt', '.png')\n",
        "        stride_h = 10\n",
        "        stride_w = 3\n",
        "        for i in range(len(anchors)):\n",
        "            (w, h) = map(float, anchors[i])\n",
        "\n",
        "            w = int(w * stride)\n",
        "            h = int(h * stride)\n",
        "            print(w, h)\n",
        "\n",
        "            # make sure starting coordinates of anchors do not overlap each other\n",
        "            offset_x = 10 + i * stride_w\n",
        "            offset_y = 10 + i * stride_h\n",
        "\n",
        "            cv2.rectangle(blank_image, (offset_x, offset_y), (offset_x + w, offset_y + h), colors[i], 2)\n",
        "\n",
        "#             cv2.imshow('%d_anchors' % int(i+1), blank_image)\n",
        "            cv2.waitKey(1000)\n",
        "            cv2.imwrite(output_img, blank_image)\n",
        "        plt.imshow(blank_image)\n",
        "        plt.title('%d_anchors' % int(i+1))\n",
        "        plt.show()\n",
        "        print('')\n",
        "\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "\n",
        "wn_in_mobilenet = [224, 224]\n",
        "dataset_disk = '/content/drive/My Drive/yeni_proje/data/'\n",
        "visualize(wn_in_mobilenet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7PiFZ2TvFa9"
      },
      "source": [
        "##Generate Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0unMV1FotbG"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def process_data(img_size, boxes):\n",
        "    \"\"\" Box preprocessing: based on two diagonal coordinates convert box info to boxcenter_x, boxcenter_y, w, h\n",
        "    and find the maximum number of boxes then do padding for all the boxes based on the maximum #boxes\n",
        "    :param boxes: array with pure box diagonal coordinates info from train_uec100.txt\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Original boxes stored as 1D list of class, x_min, y_min, x_max, y_max.\n",
        "    boxes = [box.reshape((-1, 5)) for box in boxes]\n",
        "\n",
        "    # Get box parameters as x_center, y_center, box_width, box_height, class.\n",
        "    boxes_xy = [0.5 * (box[:, 3:5] + box[:, 1:3]) for box in boxes]\n",
        "    boxes_wh = [box[:, 3:5] - box[:, 1:3] for box in boxes]\n",
        "    boxes_xy = [boxxy / img_size for boxxy in boxes_xy]\n",
        "    boxes_wh = [boxwh / img_size for boxwh in boxes_wh]\n",
        "    boxes = [np.concatenate((boxes_xy[i], boxes_wh[i], box[:, 0:1]), axis=1) for i, box in enumerate(boxes)]\n",
        "\n",
        "    # find the max number of boxes\n",
        "    max_boxes = 0\n",
        "    for box in boxes:\n",
        "        if box.shape[0] > max_boxes:\n",
        "            max_boxes = box.shape[0]\n",
        "\n",
        "    # add zero pad for training\n",
        "    for i, box in enumerate(boxes):\n",
        "        if box.shape[0] < max_boxes:\n",
        "            zero_padding = np.zeros((max_boxes - box.shape[0], 5), dtype=np.float32)\n",
        "            boxes[i] = np.vstack((box, zero_padding))\n",
        "\n",
        "    return np.array(boxes)\n",
        "\n",
        "def create_label_dict(class_path):\n",
        "    print('\\n-> creating dictinary for labels...\\n')\n",
        "    label_dict = {}\n",
        "    with open(class_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    for i in range(0, len(class_names)):\n",
        "        label_dict[class_names[i][:-1]] = i\n",
        "    return label_dict\n",
        "\n",
        "def create_label_dict(class_path):\n",
        "    print('\\n-> creating dictinary for labels...\\n')\n",
        "    label_dict = {}\n",
        "    with open(class_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    for i in range(0, len(class_names)):\n",
        "        label_dict[class_names[i][:-1]] = i\n",
        "    return label_dict\n",
        "\n",
        "def txt2data(txt_path):\n",
        "    \"\"\" Read train_uec100.txt file and convert to image_data array\n",
        "    :return: image_data array with ['Volumes/JS/UECFOOD100_JS/1/1.jpg', [0,0,143,370,486]] kind of entries\n",
        "    \"\"\"\n",
        "    print('\\n-> converting txt info to data...\\n')\n",
        "\n",
        "    # Read train_uec100.txt file and save to a dict with directory as the key, bbox as value\n",
        "    with open(txt_path, 'r') as f:\n",
        "        entries = f.readlines()\n",
        "        out = {}\n",
        "        for i, entry in enumerate(entries):\n",
        "            if i > 0:  # skip header\n",
        "                entry = entry[:-1].split(' ')\n",
        "                assert Image.open(entry[0]).size == (800, 600)  # after preprocessing size should be exactly 800,600\n",
        "                entry[2] = ' '.join(entry[2:])\n",
        "                entry[1] = str(int(entry[1]) - 1) + ' ' + entry[2]  # YOLO requires category id starts from 0 not 1\n",
        "                entry = entry[:2]\n",
        "                if entry[0] in out.keys():\n",
        "                    out[entry[0]].append(entry[1])\n",
        "                else:\n",
        "                    out[entry[0]] = [entry[1]]\n",
        "\n",
        "    # Save img directory with bbox info from out dict to image_data array\n",
        "    image_data = list()\n",
        "    index = 0\n",
        "    for k, v in out.items():\n",
        "        image_data.append([k])\n",
        "        for i in v:\n",
        "            image_data[index].append(i)\n",
        "        index += 1\n",
        "\n",
        "    # Convert string to int or float and save in image_data array again\n",
        "    for no, entry in enumerate(image_data):\n",
        "        for i, box in enumerate(entry):\n",
        "            if i != 0:      # skip img path\n",
        "                box = box.split(' ')\n",
        "                box[0] = int(box[0])  # convert class name to numbers (0~)\n",
        "\n",
        "                for k in range(1, 5):  # Change box boundaries from str to int\n",
        "                    box[k] = int(float(box[k]))\n",
        "\n",
        "                image_data[no][i] = box\n",
        "    return image_data\n",
        "\n",
        "def load_images(image_data):\n",
        "    \"\"\" Load images based on their directory in the image_data array and save them in images then return\n",
        "    :param image_data: acquired from txt2data()\n",
        "    :return: images with each img info with shape(600, 800, 3) for each\n",
        "    \"\"\"\n",
        "    print('\\n -> Reading imgs and saving to array images...\\n')\n",
        "    images = []\n",
        "    boxes = np.array([np.array(image_data[i][1:]) for i in range(np.array(image_data).shape[0])])\n",
        "    image_data = np.array(image_data)\n",
        "    boxes = process_data(img_size, boxes)\n",
        "    detectors_mask, matching_true_boxes = get_detector_mask(boxes, anchors)\n",
        "\n",
        "    for i, data in enumerate(image_data):\n",
        "        img = Image.open(os.path.join(data[0]))\n",
        "        assert img.size == (800, 600)\n",
        "        img = np.array(img, dtype=np.uint8)\n",
        "        images.append(img)\n",
        "        boxes = np.array(image_data[i][1:])\n",
        "        boxes = np.array(boxes)\n",
        "    return images\n",
        "\n",
        "def images2npv(images, image_data, shuffle=False):\n",
        "    \"\"\" Save image info and box info to npv file\n",
        "    :param images: image\n",
        "    :param image_data:\n",
        "    :param shuffle: if shuffle or not (data has been shuffled in during preprocessing)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print('\\n -> converting image info to npv file...\\n')\n",
        "    images = np.array(images, dtype=np.uint8)\n",
        "    image_data = [np.array(image_data[i][1:]) for i in range(images.shape[0])]\n",
        "    image_data = np.array(image_data)\n",
        "\n",
        "    # shuffle dataset\n",
        "    if shuffle:\n",
        "        np.random.seed(13)\n",
        "        indices = np.arange(len(images))\n",
        "        np.random.shuffle(indices)\n",
        "        images, image_data = images[indices], image_data[indices]\n",
        "    print('dataset contains {} images'.format(images.shape[0]))\n",
        "    np.savez('UECFOOD100npv_JS', image=images, boxes=image_data)\n",
        "    print('npz file has been generated and saved as UECFOOD100npv_JS.npz')\n",
        "\n",
        "\n",
        "def get_detector_mask(boxes, anchors):\n",
        "    detectors_mask = [0 for i in range(len(boxes))]\n",
        "    matching_true_boxes = [0 for i in range(len(boxes))]\n",
        "    for i, box in enumerate(boxes):\n",
        "        detectors_mask[i], matching_true_boxes[i] = preprocess_true_boxes(box, anchors, [416, 416])\n",
        "\n",
        "    return np.array(detectors_mask), np.array(matching_true_boxes)\n",
        "\n",
        "\n",
        "def preprocess_true_boxes(true_boxes, anchors, image_size):\n",
        "    \"\"\"Find detector in YOLO where ground truth box should appear\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    true_boxes : array\n",
        "        List of ground truth boxes in form of relative x, y, w, h, class.\n",
        "        Relative coordinates are in the range [0, 1] indicating a percentage\n",
        "        of the original image dimensions.\n",
        "    anchors : array\n",
        "        List of anchors in form of w, h.\n",
        "        Anchors are assumed to be in the range [0, conv_size] where conv_size\n",
        "        is the spatial dimension of the final convolutional features.\n",
        "    image_size : array-like\n",
        "        List of image dimensions in form of h, w in pixels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    detectors_mask : array\n",
        "        0/1 mask for detectors in [conv_height, conv_width, num_anchors, 1]\n",
        "        that should be compared with a matching ground truth box.\n",
        "    matching_true_boxes: array\n",
        "        Same shape as detectors_mask with the corresponding ground truth box\n",
        "        adjusted for comparison with predicted parameters at training time.\n",
        "    \"\"\"\n",
        "    height, width = image_size\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    # Downsampling factor of 5x 2-stride max_pools == 32.\n",
        "    assert height % 32 == 0,    'Image sizes in YOLO_v2 must be multiples of 32.'\n",
        "    assert width % 32 == 0,     'Image sizes in YOLO_v2 must be multiples of 32.'\n",
        "    conv_height = height // 32\n",
        "    conv_width = width // 32\n",
        "    num_box_params = true_boxes.shape[1]\n",
        "    detectors_mask = np.zeros(\n",
        "        (conv_height, conv_width, num_anchors, 1), dtype=np.float32)\n",
        "    matching_true_boxes = np.zeros(\n",
        "        (conv_height, conv_width, num_anchors, num_box_params),\n",
        "        dtype=np.float32)\n",
        "\n",
        "    for box in true_boxes:\n",
        "        # scale box to convolutional feature spatial dimensions\n",
        "        box_class = box[4:5]\n",
        "        box = box[0:4] * np.array(\n",
        "            [conv_width, conv_height, conv_width, conv_height])\n",
        "        i = np.floor(box[1]).astype('int')\n",
        "        j = np.floor(box[0]).astype('int')\n",
        "        if j >= 13 or i >= 13:\n",
        "            print('bug')\n",
        "        best_iou = 0\n",
        "        best_anchor = 0\n",
        "        for k, anchor in enumerate(anchors):\n",
        "            # Find IOU between box shifted to origin and anchor box.\n",
        "            box_maxes = box[2:4] / 2.\n",
        "            box_mins = -box_maxes\n",
        "            anchor_maxes = (anchor / 2.)\n",
        "            anchor_mins = -anchor_maxes\n",
        "\n",
        "            intersect_mins = np.maximum(box_mins, anchor_mins)\n",
        "            intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n",
        "            intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "            intersect_area = intersect_wh[0] * intersect_wh[1]\n",
        "            box_area = box[2] * box[3]\n",
        "            anchor_area = anchor[0] * anchor[1]\n",
        "            iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
        "            if iou > best_iou:\n",
        "                best_iou = iou\n",
        "                best_anchor = k\n",
        "\n",
        "        if best_iou > 0:\n",
        "            print(i, j, best_anchor)\n",
        "            detectors_mask[i, j, best_anchor] = 1\n",
        "            adjusted_box = np.array(\n",
        "                [\n",
        "                    box[0] - j, box[1] - i,\n",
        "                    np.log(box[2] / anchors[best_anchor][0]),\n",
        "                    np.log(box[3] / anchors[best_anchor][1]), box_class\n",
        "                ],\n",
        "                dtype=np.float32)\n",
        "            matching_true_boxes[i, j, best_anchor] = adjusted_box\n",
        "    return detectors_mask, matching_true_boxes\n",
        "\n",
        "anchors = []\n",
        "with open('/content/drive/My Drive/yeni_proje/data/generated_anchors_mobilenet/anchors_5.txt', 'r') as anchor_file:\n",
        "    for i, line in enumerate(anchor_file):\n",
        "        line = line.rstrip('\\n')\n",
        "        anchors.append(list(map(float, line.split(', '))))\n",
        "anchors = np.array(anchors)\n",
        "print('-> anchors acquired\\n')\n",
        "print(anchors)\n",
        "\n",
        "txt_path = './data/train_uec100.txt'\n",
        "img_size = np.array([800, 600])\n",
        "\n",
        "# Generate dictionary with labels and ids (not necessary)\n",
        "label_dict = create_label_dict('/content/drive/My Drive/yeni_proje/data/classes.txt')\n",
        "print(label_dict)\n",
        "\n",
        "# Convert txt info to data\n",
        "image_data = txt2data(txt_path)\n",
        "images = load_images(image_data)\n",
        "images2npv(images, image_data)\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxOL807dxVYd"
      },
      "source": [
        "##Annotation (XML files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TafDAFmxotd5"
      },
      "source": [
        "from PIL import Image\n",
        "from lxml import etree as ET\n",
        "\n",
        "def gen_template():\n",
        "    root = ET.Element('annotation', verified='yes')\n",
        "\n",
        "    folder = ET.SubElement(root, 'folder')\n",
        "    folder.text = 'data/1'\n",
        "\n",
        "    filename = ET.SubElement(root, 'filename')\n",
        "    filename.text = '1.jpg'\n",
        "\n",
        "    path = ET.SubElement(root, 'path')\n",
        "    path.text = '/content/drive/My Drive/proje/data/1.jpg'\n",
        "\n",
        "    size = ET.SubElement(root, 'size')\n",
        "    width = ET.SubElement(size, 'width')\n",
        "    width.text = '800'\n",
        "    height = ET.SubElement(size, 'width')\n",
        "    height.text = '600'\n",
        "    depth = ET.SubElement(size, 'width')\n",
        "    depth.text = '3'\n",
        "\n",
        "    object = ET.SubElement(root, 'object')\n",
        "    name = ET.SubElement(object, 'name')\n",
        "    name.text = 'rice'\n",
        "\n",
        "    bndbox = ET.SubElement(object, 'bndbox')\n",
        "    xmin = ET.SubElement(bndbox, 'xmin')\n",
        "    xmin.text = '0'\n",
        "    ymin = ET.SubElement(bndbox, 'ymin')\n",
        "    ymin.text = '143'\n",
        "    xmax = ET.SubElement(bndbox, 'xmax')\n",
        "    xmax.text = '370'\n",
        "    ymax = ET.SubElement(bndbox, 'ymax')\n",
        "    ymax.text = '486'\n",
        "\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write('template.xml', pretty_print=True)\n",
        "\n",
        "\n",
        "\n",
        "def write_xml(line):\n",
        "    root = ET.Element('annotation', verified='yes')\n",
        "\n",
        "    folder = ET.SubElement(root, 'folder')\n",
        "    folder.text = '/content/drive/My Drive/proje/data/1'\n",
        "\n",
        "    filename = ET.SubElement(root, 'filename')\n",
        "    filename.text = line[0] + '.jpg'\n",
        "\n",
        "    path = ET.SubElement(root, 'path')\n",
        "    path.text = '/content/drive/My Drive/proje/data/1/' + line[0] + '.jpg'\n",
        "    assert Image.open(path.text).size == (800, 600)\n",
        "\n",
        "    size = ET.SubElement(root, 'size')\n",
        "    width = ET.SubElement(size, 'width')\n",
        "    width.text = str(Image.open(path.text).size[0])\n",
        "    height = ET.SubElement(size, 'height')\n",
        "    height.text = str(Image.open(path.text).size[1])\n",
        "    depth = ET.SubElement(size, 'depth')\n",
        "    depth.text = '3'\n",
        "\n",
        "    # if not os.path.exists(str(line[0]) + '.xml'):\n",
        "    object = ET.SubElement(root, 'object')\n",
        "    name = ET.SubElement(object, 'name')\n",
        "    name.text = 'rice'\n",
        "\n",
        "    bndbox = ET.SubElement(object, 'bndbox')\n",
        "    xmin = ET.SubElement(bndbox, 'xmin')\n",
        "    xmin.text = line[1]\n",
        "    ymin = ET.SubElement(bndbox, 'ymin')\n",
        "    ymin.text = line[2]\n",
        "    xmax = ET.SubElement(bndbox, 'xmax')\n",
        "    xmax.text = line[3]\n",
        "    ymax = ET.SubElement(bndbox, 'ymax')\n",
        "    ymax.text = line[4]\n",
        "\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(folder.text + '/annotations' + line[0] + '.xml', pretty_print=True, encoding='utf-8')\n",
        "\n",
        "\n",
        "def gen_xmls(outpath, datapath):\n",
        "    new_bb_info = 'new_bb_info.txt'\n",
        "    with open(datapath + '/' + new_bb_info, 'r') as bbox_file:\n",
        "        num_items = sum(1 for line in open(datapath + '/' + new_bb_info)) - 1  # 620 images with 626 objects in total\n",
        "        print(num_items)\n",
        "        for i, line in enumerate(bbox_file):\n",
        "            if i > 0:\n",
        "                line = line.rstrip('\\n')\n",
        "                line = line.split(' ')\n",
        "                write_xml(line)\n",
        "    print('Done!')\n",
        "\n",
        "datapath = '/content/drive/My Drive/yeni_proje/data/1'\n",
        "outpath = datapath\n",
        "gen_xmls(outpath, datapath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLKCsk8uotgN"
      },
      "source": [
        "gen_template()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T3Zlvlootil"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN46UI4NotlA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-04M5j-9yGaO"
      },
      "source": [
        "#YOLO Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JoEkWsj0UwT"
      },
      "source": [
        "##Drive Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2t-m_J6yLH4"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMmIxf7TyLKV"
      },
      "source": [
        "#!pip install tensorflow==2.3.0\n",
        "#!pip install tf_nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iow6jXrlyLNB"
      },
      "source": [
        "import tensorflow as tf\n",
        "%cd /content/drive/My Drive/yeni_proje/\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDpELqwYyRAh"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5ieCySO01fC"
      },
      "source": [
        "##Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31rA3tskyRC7"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BoundBox:\n",
        "    def __init__(self, xmin, ymin, xmax, ymax, c=None, classes=None):\n",
        "        self.xmin = xmin\n",
        "        self.ymin = ymin\n",
        "        self.xmax = xmax\n",
        "        self.ymax = ymax\n",
        "\n",
        "        self.c = c\n",
        "        self.classes = classes\n",
        "\n",
        "        self.label = -1\n",
        "        self.score = -1\n",
        "\n",
        "    def get_label(self):\n",
        "        if self.label == -1:\n",
        "            self.label = np.argmax(self.classes)\n",
        "\n",
        "        return self.label\n",
        "\n",
        "    def get_score(self):\n",
        "        if self.score == -1:\n",
        "            self.score = self.classes[self.get_label()]\n",
        "\n",
        "        return self.score\n",
        "\n",
        "    def get_conf(self):\n",
        "\n",
        "        return self.c\n",
        "\n",
        "\n",
        "class WeightReader:\n",
        "    def __init__(self, weight_file):\n",
        "        self.offset = 4\n",
        "        self.all_weights = np.fromfile(weight_file, dtype='float32')\n",
        "\n",
        "    def read_bytes(self, size):\n",
        "        self.offset = self.offset + size\n",
        "        return self.all_weights[self.offset - size:self.offset]\n",
        "\n",
        "    def reset(self):\n",
        "        self.offset = 4\n",
        "\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
        "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
        "\n",
        "    intersect = intersect_w * intersect_h\n",
        "\n",
        "    w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin\n",
        "    w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin\n",
        "\n",
        "    union = w1 * h1 + w2 * h2 - intersect\n",
        "\n",
        "    return float(intersect) / union\n",
        "\n",
        "\n",
        "def decode_netout(netout, anchors, nb_class, obj_threshold=0.3, nms_threshold=0.3):\n",
        "    grid_h, grid_w, nb_box = netout.shape[:3]\n",
        "\n",
        "    boxes = []\n",
        "\n",
        "    # decode the output by the network\n",
        "    netout[..., 4] = _sigmoid(netout[..., 4])\n",
        "    netout[..., 5:] = netout[..., 4][..., np.newaxis] * _softmax(netout[..., 5:])\n",
        "    netout[..., 5:] *= netout[..., 5:] > obj_threshold\n",
        "\n",
        "    for row in range(grid_h):\n",
        "        for col in range(grid_w):\n",
        "            for b in range(nb_box):\n",
        "                # from 4th element onwards are confidence and class classes\n",
        "                classes = netout[row, col, b, 5:]\n",
        "\n",
        "                if np.sum(classes) > 0:\n",
        "                    # first 4 elements are x, y, w, and h\n",
        "                    x, y, w, h = netout[row, col, b, :4]\n",
        "\n",
        "                    x = (col + _sigmoid(x)) / grid_w  # center position, unit: image width\n",
        "                    y = (row + _sigmoid(y)) / grid_h  # center position, unit: image height\n",
        "                    w = anchors[2 * b + 0] * np.exp(w) / grid_w  # unit: image width\n",
        "                    h = anchors[2 * b + 1] * np.exp(h) / grid_h  # unit: image height\n",
        "                    confidence = netout[row, col, b, 4]\n",
        "\n",
        "                    box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, confidence, classes)\n",
        "\n",
        "                    boxes.append(box)\n",
        "\n",
        "    # suppress non-maximal boxes\n",
        "    for c in range(nb_class):\n",
        "        sorted_indices = list(reversed(np.argsort([box.classes[c] for box in boxes])))\n",
        "\n",
        "        for i in range(len(sorted_indices)):\n",
        "            index_i = sorted_indices[i]\n",
        "\n",
        "            if boxes[index_i].classes[c] == 0:\n",
        "                continue\n",
        "            else:\n",
        "                for j in range(i + 1, len(sorted_indices)):\n",
        "                    index_j = sorted_indices[j]\n",
        "\n",
        "                    if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n",
        "                        boxes[index_j].classes[c] = 0\n",
        "\n",
        "    # remove the boxes which are less likely than a obj_threshold\n",
        "    boxes = [box for box in boxes if box.get_score() > obj_threshold]\n",
        "\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def draw_boxes(image, boxes, labels):\n",
        "    image_h, image_w, _ = image.shape\n",
        "\n",
        "    for box in boxes:\n",
        "        xmin = int(box.xmin * image_w)\n",
        "        ymin = int(box.ymin * image_h)\n",
        "        xmax = int(box.xmax * image_w)\n",
        "        ymax = int(box.ymax * image_h)\n",
        "\n",
        "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 3)\n",
        "        cv2.putText(image,\n",
        "                    labels[box.get_label()] + ' ' + str(box.get_score()),\n",
        "                    (xmin, ymin - 13),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    1e-3 * image_h,\n",
        "                    (0, 255, 0), 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def compute_overlap(a, b):\n",
        "    \"\"\"\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "    Parameters\n",
        "    ----------\n",
        "    a: (N, 4) ndarray of float\n",
        "    b: (K, 4) ndarray of float\n",
        "    Returns\n",
        "    -------\n",
        "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
        "    \"\"\"\n",
        "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
        "\n",
        "    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
        "    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
        "\n",
        "    iw = np.maximum(iw, 0)\n",
        "    ih = np.maximum(ih, 0)\n",
        "\n",
        "    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
        "\n",
        "    ua = np.maximum(ua, np.finfo(float).eps)\n",
        "\n",
        "    intersection = iw * ih\n",
        "\n",
        "    return intersection / ua\n",
        "\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list).\n",
        "        precision: The precision curve (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "    # correct AP calculation\n",
        "    # first append sentinel values at the end\n",
        "    mrec = np.concatenate(([0.], recall, [1.]))\n",
        "    mpre = np.concatenate(([0.], precision, [0.]))\n",
        "\n",
        "    # compute the precision envelope\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # to calculate area under PR curve, look for points\n",
        "    # where X axis (recall) changes value\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "    # and sum (\\Delta recall) * prec\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap\n",
        "\n",
        "\n",
        "def _interval_overlap(interval_a, interval_b):\n",
        "    x1, x2 = interval_a\n",
        "    x3, x4 = interval_b\n",
        "\n",
        "    if x3 < x1:\n",
        "        if x4 < x1:\n",
        "            return 0\n",
        "        else:\n",
        "            return min(x2, x4) - x1\n",
        "    else:\n",
        "        if x2 < x3:\n",
        "            return 0\n",
        "        else:\n",
        "            return min(x2, x4) - x3\n",
        "\n",
        "\n",
        "def _sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))\n",
        "\n",
        "\n",
        "def _softmax(x, axis=-1, t=-100.):\n",
        "    x = x - np.max(x)\n",
        "\n",
        "    if np.min(x) < t:\n",
        "        x = x / np.min(x) * t\n",
        "\n",
        "    e_x = np.exp(x)\n",
        "\n",
        "    return e_x / e_x.sum(axis, keepdims=True)\n",
        "\n",
        "\n",
        "def read_category():\n",
        "    category = []\n",
        "    with open('/content/drive/My Drive/yeni_proje/data/category.txt', 'r') as file:\n",
        "        for i, line in enumerate(file):\n",
        "            if i > 0:\n",
        "                line = line.rstrip('\\n')\n",
        "                line = line.split('\\t')\n",
        "                category.append(line[1])\n",
        "    return category\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "def parse_annotation(ann_dir, img_dir, labels=[]):\n",
        "    all_imgs = []\n",
        "    seen_labels = {}\n",
        "\n",
        "    for ann in sorted(os.listdir(ann_dir)):\n",
        "        img = {'object': []}\n",
        "\n",
        "        tree = ET.parse(ann_dir + ann)\n",
        "\n",
        "        for elem in tree.iter():\n",
        "            if 'filename' in elem.tag:\n",
        "                img['filename'] = img_dir + elem.text\n",
        "            if 'width' in elem.tag:\n",
        "                img['width'] = int(elem.text)\n",
        "            if 'height' in elem.tag:\n",
        "                img['height'] = int(elem.text)\n",
        "            if 'object' in elem.tag or 'part' in elem.tag:\n",
        "                obj = {}\n",
        "\n",
        "                for attr in list(elem):\n",
        "                    if 'name' in attr.tag:\n",
        "                        obj['name'] = attr.text\n",
        "\n",
        "                        if obj['name'] in seen_labels:\n",
        "                            seen_labels[obj['name']] += 1\n",
        "                        else:\n",
        "                            seen_labels[obj['name']] = 1\n",
        "\n",
        "                        if len(labels) > 0 and obj['name'] not in labels:\n",
        "                            break\n",
        "                        else:\n",
        "                            img['object'] += [obj]\n",
        "\n",
        "                    if 'bndbox' in attr.tag:\n",
        "                        for dim in list(attr):\n",
        "                            if 'xmin' in dim.tag:\n",
        "                                obj['xmin'] = int(round(float(dim.text)))\n",
        "                            if 'ymin' in dim.tag:\n",
        "                                obj['ymin'] = int(round(float(dim.text)))\n",
        "                            if 'xmax' in dim.tag:\n",
        "                                obj['xmax'] = int(round(float(dim.text)))\n",
        "                            if 'ymax' in dim.tag:\n",
        "                                obj['ymax'] = int(round(float(dim.text)))\n",
        "\n",
        "        if len(img['object']) > 0:\n",
        "            all_imgs += [img]\n",
        "\n",
        "    return all_imgs, seen_labels\n",
        "\n",
        "\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "from keras.utils import Sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BatchGenerator(Sequence):\n",
        "    def __init__(self, images,\n",
        "                 config,\n",
        "                 shuffle=True,\n",
        "                 jitter=True,\n",
        "                 norm=None):\n",
        "        self.generator = None\n",
        "\n",
        "        self.images = images\n",
        "        self.config = config\n",
        "\n",
        "        self.shuffle = shuffle\n",
        "        self.jitter = jitter\n",
        "        self.norm = norm\n",
        "\n",
        "        self.anchors = [BoundBox(0, 0, config['ANCHORS'][2 * i], config['ANCHORS'][2 * i + 1]) for i in\n",
        "                        range(int(len(config['ANCHORS']) // 2))]\n",
        "\n",
        "        ### augmentors by https://github.com/aleju/imgaug\n",
        "        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "        # Define our sequence of augmentation steps that will be applied to every image\n",
        "        # All augmenters with per_channel=0.5 will sample one value _per image_\n",
        "        # in 50% of all cases. In all other cases they will sample new values\n",
        "        # _per channel_.\n",
        "        self.aug_pipe = iaa.Sequential(\n",
        "            [\n",
        "                # apply the following augmenters to most images\n",
        "                # iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
        "                # iaa.Flipud(0.2), # vertically flip 20% of all images\n",
        "                # sometimes(iaa.Crop(percent=(0, 0.1))), # crop images by 0-10% of their height/width\n",
        "                sometimes(iaa.Affine(\n",
        "                    # scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
        "                    # translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n",
        "                    # rotate=(-5, 5), # rotate by -45 to +45 degrees\n",
        "                    # shear=(-5, 5), # shear by -16 to +16 degrees\n",
        "                    # order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
        "                    # cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n",
        "                    # mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
        "                )),\n",
        "                # execute 0 to 5 of the following (less important) augmenters per image\n",
        "                # don't execute all of them, as that would often be way too strong\n",
        "                iaa.SomeOf((0, 5),\n",
        "                           [\n",
        "                               # sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n",
        "                               iaa.OneOf([\n",
        "                                   iaa.GaussianBlur((0, 3.0)),  # blur images with a sigma between 0 and 3.0\n",
        "                                   iaa.AverageBlur(k=(2, 7)),\n",
        "                                   # blur image using local means with kernel sizes between 2 and 7\n",
        "                                   iaa.MedianBlur(k=(3, 11)),\n",
        "                                   # blur image using local medians with kernel sizes between 2 and 7\n",
        "                               ]),\n",
        "                               iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),  # sharpen images\n",
        "                               # iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n",
        "                               # search either for all edges or for directed edges\n",
        "                               # sometimes(iaa.OneOf([\n",
        "                               #    iaa.EdgeDetect(alpha=(0, 0.7)),\n",
        "                               #    iaa.DirectedEdgeDetect(alpha=(0, 0.7), direction=(0.0, 1.0)),\n",
        "                               # ])),\n",
        "                               iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5),\n",
        "                               # add gaussian noise to images\n",
        "                               iaa.OneOf([\n",
        "                                   iaa.Dropout((0.01, 0.1), per_channel=0.5),  # randomly remove up to 10% of the pixels\n",
        "                                   # iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
        "                               ]),\n",
        "                               # iaa.Invert(0.05, per_channel=True), # invert color channels\n",
        "                               iaa.Add((-10, 10), per_channel=0.5),\n",
        "                               # change brightness of images (by -10 to 10 of original value)\n",
        "                               iaa.Multiply((0.5, 1.5), per_channel=0.5),\n",
        "                               # change brightness of images (50-150% of original value)\n",
        "                               iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),  # improve or worsen the contrast\n",
        "                               # iaa.Grayscale(alpha=(0.0, 1.0)),\n",
        "                               # sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
        "                               # sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))) # sometimes move parts of the image around\n",
        "                           ],\n",
        "                           random_order=True\n",
        "                           )\n",
        "            ],\n",
        "            random_order=True\n",
        "        )\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(self.images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(float(len(self.images)) / self.config['BATCH_SIZE']))\n",
        "\n",
        "    def num_classes(self):\n",
        "        return len(self.config['LABELS'])\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def load_annotation(self, i):\n",
        "        annots = []\n",
        "\n",
        "        for obj in self.images[i]['object']:\n",
        "            print(obj['name'])\n",
        "            # if obj['name'][-1] == ' ':\n",
        "            #     obj['name'] = obj['name'][:-1]\n",
        "            # if obj['name'] == 'fermented soybeans':\n",
        "            #     obj['name'] = 'natto'\n",
        "            # if obj['name'] == 'beef steak':\n",
        "            #     obj['name'] = 'steak'\n",
        "            # if obj['name'] == 'rolled omelet':\n",
        "            #     obj['name'] = 'omelet'\n",
        "            annot = [obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax'], self.config['LABELS'].index(obj['name'])]\n",
        "            annots += [annot]\n",
        "\n",
        "        if len(annots) == 0:\n",
        "            annots = [[]]\n",
        "\n",
        "        return np.array(annots)\n",
        "\n",
        "    def load_image(self, i):\n",
        "        return cv2.imread(self.images[i]['filename'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        l_bound = idx * self.config['BATCH_SIZE']\n",
        "        r_bound = (idx + 1) * self.config['BATCH_SIZE']\n",
        "\n",
        "        if r_bound > len(self.images):\n",
        "            r_bound = len(self.images)\n",
        "            l_bound = r_bound - self.config['BATCH_SIZE']\n",
        "\n",
        "        instance_count = 0\n",
        "\n",
        "        x_batch = np.zeros((r_bound - l_bound, self.config['IMAGE_H'], self.config['IMAGE_W'], 3))  # input images\n",
        "        b_batch = np.zeros((r_bound - l_bound, 1, 1, 1, self.config['TRUE_BOX_BUFFER'],\n",
        "                            4))  # list of self.config['TRUE_self.config['BOX']_BUFFER'] GT boxes\n",
        "        y_batch = np.zeros((r_bound - l_bound, self.config['GRID_H'], self.config['GRID_W'], self.config['BOX'],\n",
        "                            4 + 1 + len(self.config['LABELS'])))  # desired network output\n",
        "\n",
        "        for train_instance in self.images[l_bound:r_bound]:\n",
        "            # augment input image and fix object's position and size\n",
        "            img, all_objs = self.aug_image(train_instance, jitter=self.jitter)\n",
        "\n",
        "            # construct output from object's x, y, w, h\n",
        "            true_box_index = 0\n",
        "\n",
        "            for obj in all_objs:\n",
        "                if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin'] and obj['name'] in self.config['LABELS']:\n",
        "                    center_x = .5 * (obj['xmin'] + obj['xmax'])\n",
        "                    center_x = center_x / (float(self.config['IMAGE_W']) / self.config['GRID_W'])\n",
        "                    center_y = .5 * (obj['ymin'] + obj['ymax'])\n",
        "                    center_y = center_y / (float(self.config['IMAGE_H']) / self.config['GRID_H'])\n",
        "\n",
        "                    grid_x = int(np.floor(center_x))\n",
        "                    grid_y = int(np.floor(center_y))\n",
        "\n",
        "                    if grid_x < self.config['GRID_W'] and grid_y < self.config['GRID_H']:\n",
        "                        obj_indx = self.config['LABELS'].index(obj['name'])\n",
        "\n",
        "                        center_w = (obj['xmax'] - obj['xmin']) / (\n",
        "                                float(self.config['IMAGE_W']) / self.config['GRID_W'])  # unit: grid cell\n",
        "                        center_h = (obj['ymax'] - obj['ymin']) / (\n",
        "                                float(self.config['IMAGE_H']) / self.config['GRID_H'])  # unit: grid cell\n",
        "\n",
        "                        box = [center_x, center_y, center_w, center_h]\n",
        "\n",
        "                        # find the anchor that best predicts this box\n",
        "                        best_anchor = -1\n",
        "                        max_iou = -1\n",
        "\n",
        "                        shifted_box = BoundBox(0,\n",
        "                                               0,\n",
        "                                               center_w,\n",
        "                                               center_h)\n",
        "\n",
        "                        for i in range(len(self.anchors)):\n",
        "                            anchor = self.anchors[i]\n",
        "                            iou = bbox_iou(shifted_box, anchor)\n",
        "\n",
        "                            if max_iou < iou:\n",
        "                                best_anchor = i\n",
        "                                max_iou = iou\n",
        "\n",
        "                        # assign ground truth x, y, w, h, confidence and class probs to y_batch\n",
        "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 0:4] = box\n",
        "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 4] = 1.\n",
        "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 5 + obj_indx] = 1\n",
        "\n",
        "                        # assign the true box to b_batch\n",
        "                        b_batch[instance_count, 0, 0, 0, true_box_index] = box\n",
        "\n",
        "                        true_box_index += 1\n",
        "                        true_box_index = true_box_index % self.config['TRUE_BOX_BUFFER']\n",
        "\n",
        "            # assign input image to x_batch\n",
        "            if self.norm is not None:\n",
        "                x_batch[instance_count] = self.norm(img)\n",
        "            else:\n",
        "                # plot image and bounding boxes for sanity check\n",
        "                for obj in all_objs:\n",
        "                    if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin']:\n",
        "                        cv2.rectangle(img[:, :, ::-1], (obj['xmin'], obj['ymin']), (obj['xmax'], obj['ymax']),\n",
        "                                      (255, 0, 0), 3)\n",
        "                        cv2.putText(img[:, :, ::-1], obj['name'],\n",
        "                                    (obj['xmin'] + 2, obj['ymin'] + 12),\n",
        "                                    0, 1.2e-3 * img.shape[0],\n",
        "                                    (0, 255, 0), 2)\n",
        "\n",
        "                x_batch[instance_count] = img\n",
        "\n",
        "            # increase instance counter in current batch\n",
        "            instance_count += 1\n",
        "\n",
        "            # print(' new batch created', idx)\n",
        "\n",
        "        return [x_batch, b_batch], y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle: np.random.shuffle(self.images)\n",
        "\n",
        "    def aug_image(self, train_instance, jitter):\n",
        "        image_name = train_instance['filename']\n",
        "        image = cv2.imread(image_name)\n",
        "\n",
        "        if image is None:\n",
        "            print('Cannot find ', image_name)\n",
        "\n",
        "        h, w, c = image.shape\n",
        "        all_objs = copy.deepcopy(train_instance['object'])\n",
        "\n",
        "        if jitter:\n",
        "            ### scale the image\n",
        "            scale = np.random.uniform() / 10. + 1.\n",
        "            image = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n",
        "\n",
        "            ### translate the image\n",
        "            max_offx = (scale - 1.) * w\n",
        "            max_offy = (scale - 1.) * h\n",
        "            offx = int(np.random.uniform() * max_offx)\n",
        "            offy = int(np.random.uniform() * max_offy)\n",
        "\n",
        "            image = image[offy: (offy + h), offx: (offx + w)]\n",
        "\n",
        "            ### flip the image\n",
        "            flip = np.random.binomial(1, .5)\n",
        "            if flip > 0.5:\n",
        "                image = cv2.flip(image, 1)\n",
        "\n",
        "            image = self.aug_pipe.augment_image(image)\n",
        "\n",
        "        # resize the image to standard size\n",
        "        image = cv2.resize(image, (self.config['IMAGE_H'], self.config['IMAGE_W']))\n",
        "        image = image[:, :, ::-1]\n",
        "\n",
        "        # fix object's position and size\n",
        "        for obj in all_objs:\n",
        "            for attr in ['xmin', 'xmax']:\n",
        "                if jitter:\n",
        "                    obj[attr] = int(obj[attr] * scale - offx)\n",
        "\n",
        "                # convert annotation from raw size to net input size\n",
        "                obj[attr] = int(obj[attr] * float(self.config['IMAGE_W']) / w)\n",
        "                obj[attr] = max(min(obj[attr], self.config['IMAGE_W']), 0)\n",
        "\n",
        "            for attr in ['ymin', 'ymax']:\n",
        "                if jitter:\n",
        "                    obj[attr] = int(obj[attr] * scale - offy)\n",
        "\n",
        "                # convert annotation from raw size to net input size\n",
        "                obj[attr] = int(obj[attr] * float(self.config['IMAGE_H']) / h)\n",
        "                obj[attr] = max(min(obj[attr], self.config['IMAGE_H']), 0)\n",
        "\n",
        "            if jitter and flip > 0.5:\n",
        "                xmin = obj['xmin']\n",
        "                obj['xmin'] = self.config['IMAGE_W'] - obj['xmax']\n",
        "                obj['xmax'] = self.config['IMAGE_W'] - xmin\n",
        "\n",
        "        return image, all_objs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mbCnDQA1TPB"
      },
      "source": [
        "##Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9avdPXzyRFy"
      },
      "source": [
        "import warnings\n",
        "import keras.backend as K\n",
        "import keras.utils\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "\n",
        "def relu6(x):\n",
        "    return K.relu(x, max_value=6)\n",
        "\n",
        "\n",
        "def MobileNetV1(input_shape,\n",
        "                alpha=1.0,\n",
        "                depth_multiplier=1,\n",
        "                include_top=True,\n",
        "                weights='imagenet',\n",
        "                pooling=None,\n",
        "                ):\n",
        "    if input_shape[-1] not in [1, 3]:\n",
        "        warnings.warn('Images must have 3 channels (RGB) or 1 channel')\n",
        "    assert input_shape[0] in [224, 192, 160, 128]\n",
        "    assert input_shape[1] in [224, 192, 160, 128]\n",
        "\n",
        "    ''' construct mobilenet '''\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Conv / s2: filter shape (3 x 3 x 3 x 32)\n",
        "    num_filters = int(alpha * 32)\n",
        "    x = layers.ZeroPadding2D(padding=(1, 1), name='conv1_zeropad')(inputs)\n",
        "    x = layers.Conv2D(filters=num_filters, kernel_size=(3, 3), padding='valid', use_bias=False, strides=(2, 2),\n",
        "                      name='conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=-1)(x)\n",
        "    x = layers.Activation(relu6, name='conv1_relu6')(x)\n",
        "\n",
        "    # Conv dw / s1: filter shape (3 x 3 x 32 dw)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(1, 1), depth_multiplier=depth_multiplier, id=1)\n",
        "    # Conv pw / s1: filter shape (1 x 1 x 32 x 64)\n",
        "    x = pointwise_block(x, num_filters=64, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=1)\n",
        "\n",
        "    # Conv dw / s2: filter shape (3 x 3 x 64 dw)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(2, 2), depth_multiplier=depth_multiplier, id=2)\n",
        "    # Conv pw / s1: filter shape (1 x 1 x 64 x 128)\n",
        "    x = pointwise_block(x, num_filters=128, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=2)\n",
        "\n",
        "    # Conv dw / s1: filter shape (3 x 3 x 128 dw)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(1, 1), depth_multiplier=depth_multiplier, id=3)\n",
        "    # Conv pw / s1: filter shape (1 x 1 x 128 x 128)\n",
        "    x = pointwise_block(x, num_filters=128, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=3)\n",
        "\n",
        "    # Conv dw / s2: filter shape (3 x 3 x 128 dw)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(2, 2), depth_multiplier=depth_multiplier, id=4)\n",
        "    # Conv pw / s1: filter shape (1 x 1 x 128 x 256)\n",
        "    x = pointwise_block(x, num_filters=256, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=4)\n",
        "\n",
        "    # Conv dw / s1: filter shape (3 x 3 x 256 dw)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(1, 1), depth_multiplier=depth_multiplier, id=5)\n",
        "    # Conv pw / s1: filter shape (1 x 1 x 256 x 256)\n",
        "    x = pointwise_block(x, num_filters=256, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=5)\n",
        "\n",
        "    # Conv dw / s2: filter shape (3 x 3 x 256 dw)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(2, 2), depth_multiplier=depth_multiplier, id=6)\n",
        "    # Conv pw / s1: filter shape (1 x 1 x 256 x 512)\n",
        "    x = pointwise_block(x, num_filters=512, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=6)\n",
        "\n",
        "    # repeat 5 times\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(1, 1), depth_multiplier=depth_multiplier, id=7)\n",
        "    x = pointwise_block(x, num_filters=512, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=7)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(1, 1), depth_multiplier=depth_multiplier, id=8)\n",
        "    x = pointwise_block(x, num_filters=512, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=8)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(1, 1), depth_multiplier=depth_multiplier, id=9)\n",
        "    x = pointwise_block(x, num_filters=512, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=9)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(1, 1), depth_multiplier=depth_multiplier, id=10)\n",
        "    x = pointwise_block(x, num_filters=512, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=10)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(1, 1), depth_multiplier=depth_multiplier, id=11)\n",
        "    x = pointwise_block(x, num_filters=512, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=11)\n",
        "\n",
        "    # Conv dw / s2: filter shape (3 x 3 x 512 dw)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(2, 2), depth_multiplier=depth_multiplier, id=12)\n",
        "    # Conv pw / s1: filter shape (1 x 1 x 512 x 1024)\n",
        "    x = pointwise_block(x, num_filters=1024, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=12)\n",
        "\n",
        "    # Conv dw / s2: filter shape (3 x 3 x 1024 dw)\n",
        "    x = depthwise_block(x, kernel=(3, 3), stride=(1, 1), depth_multiplier=depth_multiplier, id=13)\n",
        "    # Conv pw / s1: filter shape (1 x 1 x 1024 x 1024)\n",
        "    x = pointwise_block(x, num_filters=1024, kernel=(1, 1), stride=(1, 1), alpha=alpha, id=13)\n",
        "\n",
        "    # average pooling\n",
        "    if include_top:\n",
        "        raise NotImplementedError\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = layers.GlobalAveragePooling2D()(x)\n",
        "        elif pooling == 'max':\n",
        "            x = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "    # create model\n",
        "    rows = input_shape[0]\n",
        "    model = models.Model(inputs, x, name='mobilenet_%0.2f_%s' % (alpha, rows))\n",
        "\n",
        "    # load pre-trained weights on ImageNet\n",
        "    if weights == 'imagenet':\n",
        "        if alpha == 1.0:\n",
        "            alpha_text = '1_0'\n",
        "        elif alpha == 0.75:\n",
        "            alpha_text = '7_5'\n",
        "        elif alpha == 0.50:\n",
        "            alpha_text = '5_0'\n",
        "        else:\n",
        "            alpha_text = '2_5'\n",
        "\n",
        "        BASE_WEIGHT_PATH = ('https://github.com/fchollet/deep-learning-models/'\n",
        "                            'releases/download/v0.6/')\n",
        "        if include_top:\n",
        "            model_name = 'mobilenet_%s_%d_tf.h5' % (alpha_text, rows)\n",
        "            weight_path = BASE_WEIGHT_PATH + model_name\n",
        "            weights_path = keras.utils.get_file(model_name,\n",
        "                                                weight_path,\n",
        "                                                cache_subdir='models')\n",
        "        else:\n",
        "            model_name = 'mobilenet_%s_%d_tf_no_top.h5' % (alpha_text, rows)\n",
        "            weight_path = BASE_WEIGHT_PATH + model_name\n",
        "            weights_path = keras.utils.get_file(model_name,\n",
        "                                                weight_path,\n",
        "                                                cache_subdir='models')\n",
        "        model.load_weights(weights_path)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def depthwise_block(inputs, kernel, stride, depth_multiplier, id):\n",
        "    x = layers.ZeroPadding2D(padding=(1, 1), name='convdw_zeropad_%d' % id)(inputs)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=kernel, padding='valid', depth_multiplier=depth_multiplier, strides=stride,\n",
        "                               use_bias=False, name='convdw_%d' % id)(x)\n",
        "    x = layers.BatchNormalization(axis=-1, name='convdw_bn_%d' % id)(x)\n",
        "    return layers.Activation(relu6, name='convdw_relu6_%d' % id)(x)\n",
        "\n",
        "\n",
        "def pointwise_block(inputs, num_filters, kernel, stride, alpha, id):\n",
        "    num_filter = int(alpha * num_filters)\n",
        "    x = layers.Conv2D(filters=num_filter, padding='same', kernel_size=kernel, strides=stride, use_bias=False,\n",
        "                      name='convpw_%d' % id)(inputs)\n",
        "    x = layers.BatchNormalization(axis=-1, name='convpw_bn_%d' % id)(x)\n",
        "    return layers.Activation(relu6, name='convpw_relu6_%d' % id)(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLYzpFmp1iKm"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxhX9JPZyRIM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.layers import Reshape, Conv2D, Input, Lambda, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "def normalize(image):\n",
        "    return image / 255.\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    \"\"\" Build MobileNetV1 model \"\"\"\n",
        "    print('=> Building MobileNetV1 model...')\n",
        "    mobilenet = MobileNetV1(input_shape=(224, 224, 3), include_top=False)\n",
        "    x = mobilenet(input_image)\n",
        "    x = Conv2D(N_BOX * (4 + 1 + CLASS), (1, 1), strides=(1, 1), padding='same', name='conv_23')(x)\n",
        "    output = Reshape((GRID_H, GRID_W, N_BOX, 4 + 1 + CLASS))(x)\n",
        "\n",
        "    # small hack to allow true_boxes to be registered when Keras build the model\n",
        "    # for more information: https://github.com/fchollet/keras/issues/2790\n",
        "    output = Lambda(lambda args: args[0])([output, true_boxes])\n",
        "\n",
        "    model = Model([input_image, true_boxes], output)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(model):\n",
        "\n",
        "    layer = model.layers[-4]            # the last convolutional layer\n",
        "    weights = layer.get_weights()\n",
        "\n",
        "    new_kernel = np.random.normal(size=weights[0].shape) / (GRID_H * GRID_W)\n",
        "    new_bias = np.random.normal(size=weights[1].shape) / (GRID_H * GRID_W)\n",
        "\n",
        "    layer.set_weights([new_kernel, new_bias])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss',\n",
        "                               min_delta=0.001,\n",
        "                               patience=3,\n",
        "                               mode='min',\n",
        "                               verbose=1)\n",
        "\n",
        "    checkpoint = ModelCheckpoint('all_imgs_mobile_net_loss.h5',\n",
        "                                 monitor='val_loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 mode='min',\n",
        "                                 save_freq=1)\n",
        "\n",
        "    # model.load_weights('./models/mobile_net_loss0_07.h5')\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: try different optimizer and tweak parameters (in MNv1 paper they used RMSprop)\n",
        "    optimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "    # optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
        "    # optimizer = RMSprop(lr=1e-5, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "    model.compile(loss=custom_loss, optimizer=optimizer)\n",
        "\n",
        "    model.fit(train_batch,\n",
        "                        steps_per_epoch=len(train_batch),\n",
        "                        epochs=20,  # 100\n",
        "                        verbose=1,\n",
        "                        validation_data=valid_batch,\n",
        "                        validation_steps=len(valid_batch),\n",
        "                        callbacks=[early_stop, checkpoint],\n",
        "                        max_queue_size=3)\n",
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    mask_shape = tf.shape(y_true)[:4]\n",
        "\n",
        "    cell_x = tf.cast(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)),dtype=tf.float32)\n",
        "    cell_y = tf.transpose(cell_x, (0, 2, 1, 3, 4))\n",
        "\n",
        "    cell_grid = tf.tile(tf.concat([cell_x, cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
        "\n",
        "    coord_mask = tf.zeros(mask_shape)\n",
        "    conf_mask = tf.zeros(mask_shape)\n",
        "    class_mask = tf.zeros(mask_shape)\n",
        "\n",
        "    seen = tf.Variable(0.)\n",
        "    total_recall = tf.Variable(0.)\n",
        "\n",
        "    \"\"\" Adjust prediction \"\"\"\n",
        "    # adjust x and y\n",
        "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
        "\n",
        "    # adjust w and h\n",
        "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1, 1, 1, N_BOX, 2])\n",
        "\n",
        "    # adjust confidence\n",
        "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
        "\n",
        "    # adjust class probabilities\n",
        "    pred_box_class = y_pred[..., 5:]\n",
        "\n",
        "    \"\"\" Adjust ground truth \"\"\"\n",
        "    # adjust x and y\n",
        "    true_box_xy = y_true[..., 0:2]  # relative position to the containing cell\n",
        "\n",
        "    # adjust w and h\n",
        "    true_box_wh = y_true[..., 2:4]  # number of cells accross, horizontally and vertically\n",
        "\n",
        "    # adjust confidence\n",
        "    true_wh_half = true_box_wh / 2.\n",
        "    true_mins = true_box_xy - true_wh_half\n",
        "    true_maxes = true_box_xy + true_wh_half\n",
        "\n",
        "    pred_wh_half = pred_box_wh / 2.\n",
        "    pred_mins = pred_box_xy - pred_wh_half\n",
        "    pred_maxes = pred_box_xy + pred_wh_half\n",
        "\n",
        "    intersect_mins = tf.maximum(pred_mins, true_mins)\n",
        "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "\n",
        "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
        "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores = tf.truediv(intersect_areas, union_areas)\n",
        "\n",
        "    true_box_conf = iou_scores * y_true[..., 4]\n",
        "\n",
        "    # adjust class probabilities\n",
        "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
        "\n",
        "    \"\"\" Determine the masks \"\"\"\n",
        "    # coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
        "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
        "\n",
        "    # confidence mask: penelize predictors + penalize boxes with low IOU\n",
        "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
        "    true_xy = true_boxes[..., 0:2]\n",
        "    true_wh = true_boxes[..., 2:4]\n",
        "\n",
        "    true_wh_half = true_wh / 2.\n",
        "    true_mins = true_xy - true_wh_half\n",
        "    true_maxes = true_xy + true_wh_half\n",
        "\n",
        "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
        "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
        "\n",
        "    pred_wh_half = pred_wh / 2.\n",
        "    pred_mins = pred_xy - pred_wh_half\n",
        "    pred_maxes = pred_xy + pred_wh_half\n",
        "\n",
        "    intersect_mins = tf.maximum(pred_mins, true_mins)\n",
        "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "\n",
        "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
        "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores = tf.truediv(intersect_areas, union_areas)\n",
        "\n",
        "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
        "    conf_mask = conf_mask + tf.cast(best_ious < 0.6,dtype=tf.float32) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
        "\n",
        "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
        "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
        "\n",
        "    # class mask: simply the position of the ground truth boxes (the predictors)\n",
        "    class_mask = y_true[..., 4] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE\n",
        "\n",
        "    \"\"\" Warm-up training \"\"\"\n",
        "    no_boxes_mask = tf.cast(coord_mask < COORD_SCALE / 2.,dtype=tf.float32)\n",
        "    seen = tf.compat.v1.assign_add(seen,1.)\n",
        "\n",
        "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES),\n",
        "                                                   lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask,\n",
        "                                                            true_box_wh + tf.ones_like(true_box_wh) * np.reshape(\n",
        "                                                                ANCHORS, [1, 1, 1, N_BOX, 2]) * no_boxes_mask,\n",
        "                                                            tf.ones_like(coord_mask)],\n",
        "                                                   lambda: [true_box_xy,\n",
        "                                                            true_box_wh,\n",
        "                                                            coord_mask])\n",
        "\n",
        "    \"\"\" Finalize the loss \"\"\"\n",
        "    nb_coord_box = tf.reduce_sum(tf.cast(coord_mask > 0.0,dtype=tf.float32))\n",
        "    nb_conf_box = tf.reduce_sum(tf.cast(conf_mask > 0.0,dtype=tf.float32))\n",
        "    nb_class_box = tf.reduce_sum(tf.cast(class_mask > 0.0,dtype=tf.float32))\n",
        "\n",
        "    loss_xy = tf.reduce_sum(tf.square(true_box_xy - pred_box_xy) * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_wh = tf.reduce_sum(tf.square(true_box_wh - pred_box_wh) * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_conf = tf.reduce_sum(tf.square(true_box_conf - pred_box_conf) * conf_mask) / (nb_conf_box + 1e-6) / 2.\n",
        "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
        "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
        "\n",
        "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
        "\n",
        "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
        "    nb_pred_box = tf.reduce_sum(tf.cast(true_box_conf > 0.5,dtype=tf.float32) * tf.cast(pred_box_conf > 0.3,dtype=tf.float32))\n",
        "    return loss\n",
        "    \"\"\" Debugging code \"\"\"\n",
        "\"\"\"    current_recall = nb_pred_box / (nb_true_box + 1e-6)\n",
        "    total_recall = tf.compat.v1.assign_add(total_recall, current_recall)\n",
        "\n",
        "    loss = tf.compat.v1.Print(loss, [tf.zeros((1))], message='\\nDummy Line \\t', summarize=1000)\n",
        "    loss = tf.compat.v1.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
        "    loss = tf.compat.v1.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
        "    loss = tf.compat.v1.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
        "    loss = tf.compat.v1.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
        "    loss = tf.compat.v1.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
        "    loss = tf.compat.v1.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
        "    loss = tf.compat.v1.Print(loss, [total_recall / seen], message='Average Recall \\t', summarize=1000)\n",
        "\"\"\"\n",
        "    \n",
        "\n",
        "\n",
        "def read_category():\n",
        "    category = []\n",
        "    with open('/content/drive/My Drive/yeni_proje/data/category.txt', 'r') as file:\n",
        "        for i, line in enumerate(file):\n",
        "            if i > 0:\n",
        "                line = line.rstrip('\\n')\n",
        "                line = line.split('\\t')\n",
        "                category.append(line[1])\n",
        "    return category\n",
        "\n",
        "\n",
        "def plt_example_batch(batches, batch_size=16):\n",
        "    assert batches[0][0][0].shape[0] == batch_size       # in general 16x224x224x3\n",
        "    for i in range(0, batch_size):\n",
        "        img = batches[0][0][0][i]\n",
        "        plt.figure(i)\n",
        "        plt.imshow(img.astype('uint8'))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    ''' Initiailize parameters '''\n",
        "    LABELS = read_category()\n",
        "\n",
        "    IMAGE_H, IMAGE_W = 224, 224  # must equal to GRID_H * 32  416, 416\n",
        "    GRID_H, GRID_W = 7, 7        # 13, 13\n",
        "    N_BOX = 5\n",
        "    CLASS = len(LABELS)\n",
        "    CLASS_WEIGHTS = np.ones(CLASS, dtype='float32')\n",
        "    OBJ_THRESHOLD = 0.3\n",
        "    NMS_THRESHOLD = 0.3\n",
        "\n",
        "    # Read knn generated anchor_5.txt\n",
        "    ANCHORS = []\n",
        "    with open('/content/drive/My Drive/yeni_proje/data/generated_anchors_mobilenet/anchors_5.txt', 'r') as anchor_file:\n",
        "        for i, line in enumerate(anchor_file):\n",
        "            line = line.rstrip('\\n')\n",
        "            ANCHORS.append(list(map(float, line.split(', '))))\n",
        "    ANCHORS = list(list(np.array(ANCHORS).reshape(1, -1))[0])\n",
        "\n",
        "    NO_OBJECT_SCALE = 1.0\n",
        "    OBJECT_SCALE = 5.0\n",
        "    COORD_SCALE = 1.0\n",
        "    CLASS_SCALE = 1.0\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    WARM_UP_BATCHES = 10\n",
        "    TRUE_BOX_BUFFER = 10\n",
        "\n",
        "    generator_config = {\n",
        "        'IMAGE_H': IMAGE_H,\n",
        "        'IMAGE_W': IMAGE_W,\n",
        "        'GRID_H': GRID_H,\n",
        "        'GRID_W': GRID_W,\n",
        "        'BOX': N_BOX,\n",
        "        'LABELS': LABELS,\n",
        "        'CLASS': len(LABELS),\n",
        "        'ANCHORS': ANCHORS,\n",
        "        'BATCH_SIZE': BATCH_SIZE,\n",
        "        'TRUE_BOX_BUFFER': TRUE_BOX_BUFFER,\n",
        "    }\n",
        "\n",
        "    all_imgs = []\n",
        "    for i in range(0, len(LABELS)):\n",
        "        image_path = '/content/drive/My Drive/yeni_proje/data/' + str(i+1) + '/'\n",
        "        annot_path = '/content/drive/My Drive/yeni_proje/data/' + str(i+1) + '/' + '/annotations/'\n",
        "\n",
        "        folder_imgs, seen_labels = parse_annotation(annot_path, image_path)\n",
        "        all_imgs.extend(folder_imgs)\n",
        "    print(np.array(all_imgs).shape)\n",
        "\n",
        "    # add extensions to image name\n",
        "    for img in all_imgs:\n",
        "        img['filename'] = img['filename']\n",
        "\n",
        "    print('=> Generate BatchGenerator.')\n",
        "    batches = BatchGenerator(all_imgs, generator_config)\n",
        "\n",
        "    ''' Start training '''\n",
        "    train_valid_split = int(0.8 * len(all_imgs))\n",
        "\n",
        "    train_batch = BatchGenerator(all_imgs[:train_valid_split], generator_config, norm=normalize, jitter=True)\n",
        "    valid_batch = BatchGenerator(all_imgs[train_valid_split:], generator_config, norm=normalize, jitter=True)\n",
        "\n",
        "    input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "    true_boxes = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER, 4))\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    train(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbIBUbl_11jK"
      },
      "source": [
        "##Detecting Food"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QepLDHeyRLd"
      },
      "source": [
        "model.load_weights('all_imgs_mobile_net_loss.h5')\n",
        "dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
        "\n",
        "image = cv2.imread('/content/drive/My Drive/yeni_proje/test.jpg')\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "input_image = cv2.resize(image, (224, 224))\n",
        "input_image = input_image / 255.\n",
        "input_image = input_image[:,:,::-1]\n",
        "input_image = np.expand_dims(input_image, 0)\n",
        "\n",
        "netout = model.predict([input_image, dummy_array])\n",
        "\n",
        "boxes = decode_netout(netout[0], \n",
        "                      obj_threshold=0.3,\n",
        "                      nms_threshold=NMS_THRESHOLD,\n",
        "                      anchors=ANCHORS, \n",
        "                      nb_class=CLASS)\n",
        "image = draw_boxes(image, boxes, labels=LABELS)\n",
        "\n",
        "plt.imshow(image[:,:,::-1])\n",
        "plt.imsave('mn_result_0717.png', image[:,:,::-1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtuG4OmfyRNw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hycZOGaMyRQl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5gB0g58yRTb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}